import streamlit as st
import pandas as pd
import plotly.express as px 

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error

import mlflow
import mlflow.sklearn
from mlflow.models import infer_signature

from prometheus_client import Gauge, make_wsgi_app, REGISTRY
from werkzeug.middleware.dispatcher import DispatcherMiddleware
from flask import Flask
import threading

import datetime

import shutil
import os
import csv

# Librairie pour le t√©l√©chargement depuis Google Drive
import gdown

# Librairie pour mesurer le temps √©coul√©
import time

# Pour la requ√™te SQL
import urllib.parse

# Pour le lancement de commande bash
import subprocess

# Pour le lancement des requ√™tes vers l'API
import requests

# Pour la gestion de l'authentification de l'API
import zipfile
from dotenv import load_dotenv

# D√©claration des m√©triques Prometheus (exposer ensuite sur /metrics)
try: # try pour √©viter un bug a cause d'une red√©claration quand le script est relanc√© automatiquement par Streamlit (√† chaque interaction avec l‚Äôinterface Streamlit r√©ex√©cute tout le script Python depuis le d√©but)
    r2_gauge = Gauge('model_r2_score', 'Score R¬≤ du dernier mod√®le') # model_r2_score : dernier score R¬≤ du mod√®le
    rmse_gauge = Gauge('model_rmse', 'Erreur RMSE du dernier mod√®le') # model_rmse : derni√®re erreur RMSE du mod√®le
except ValueError: # Si les m√©triques ont d√©j√† √©t√© cr√©√©es (ex : rechargement Streamlit) on les r√©cup√®re depuis le registre global Prometheus sinon bug 
    r2_gauge = REGISTRY._names_to_collectors['model_r2_score']
    rmse_gauge = REGISTRY._names_to_collectors['model_rmse']

# Serveur Prometheus / d√©marrage d‚Äôun petit serveur HTTP interne via Flask
def start_prometheus_server(): # le serveur expose un endpoint /metrics pour  permettre √† Prometheus de scraper les m√©triques R¬≤ et RMSE
    app = Flask(__name__) # Cr√©ation d'une app Flask
    app.wsgi_app = DispatcherMiddleware(app.wsgi_app, { # Permet de d√©l√©guer les requ√™tes vers /metrics √† l'app WSGI de Prometheus
        "/metrics": make_wsgi_app()
    })
    app.run(host="0.0.0.0", port=8001) # Lancement du serveur Flask sur le port 8001, accessible pour Prometheus car dans le m√™me r√©seau Docker

# Permet d'√©viter de lancer plusieurs fois le serveur Flask a chaque rechargements automatiques de Streamlit sinon cela provoque l'erreur "port d√©j√† utilis√©"
if "metrics_server_started" not in st.session_state:
    threading.Thread(target=start_prometheus_server, daemon=True).start() # le serveur flash utilise un thread "daemon" pour qu'il s'ex√©cute en arri√®re-plan sans bloquer l'app principale
    st.session_state.metrics_server_started = True

@st.cache_data
# Fonction permettant de lancer la Pipeline DVC
def run_dvc_repro():
    try:
        result = subprocess.run(['dvc', 'repro', '--force'], check=True, capture_output=True, text=True)
        st.success('‚úÖ Commande ex√©cut√©e avec succ√®s !')
        st.text(result.stdout)
    except subprocess.CalledProcessError as e:
        st.error("‚ùå Une erreur est survenue lors de l'ex√©cution de la commande DVC.")
        st.text(e.stderr)

# Fonction pour charger et pr√©traiter le dataset
def load_and_preprocess_data():
    # Chargement du dataset :
    df = pd.read_csv('data/processed/DF_Processed.csv')
    
    # Suppression des espaces dans les noms de colonnes
    df.columns = df.columns.str.strip()
    
    # S√©lection des features et de la target
    baseline_features = ['M (kg)', 'Ec (cm3)', 'Ep (KW)', 'Erwltp (g/km)', 'Fc', 'Ft_Diesel', 'Ft_Essence']
    target = 'Ewltp (g/km)'
    
    X_baseline = df[baseline_features]
    y_baseline = df[target]
    
    # Split en train/test
    X_train, X_test, y_train, y_test = train_test_split(X_baseline, y_baseline, test_size=0.2)
    
    return df, X_train, X_test, y_train, y_test

# Fonction permettant un affichage dynamique de la taille du fichier
def format_size(bytes_size):
    """Formatage dynamique des tailles en KB, MB ou GB."""
    if bytes_size < 1024:
        return f"{bytes_size:.2f} B"
    elif bytes_size < 1024 ** 2:
        return f"{bytes_size / 1024:.2f} KB"
    elif bytes_size < 1024 ** 3:
        return f"{bytes_size / (1024 ** 2):.2f} MB"
    else:
        return f"{bytes_size / (1024 ** 3):.2f} GB"

def move_csv_files(src_folder, dest_folder):
    """
    D√©place tous les fichiers CSV du dossier racine vers le dossier de destination.

    :param src_folder: Chemin du dossier source.
    :param dest_folder: Chemin du dossier de destination.
    """
    # V√©rifier l'existence du dossier source
    if not os.path.exists(src_folder):
        st.write(f"‚ùå Le dossier source '{src_folder}' n'existe pas.")
        return

    # Cr√©er le dossier de destination s'il n'existe pas
    os.makedirs(dest_folder, exist_ok=True)

    # Parcourir uniquement le dossier racine (sans sous-dossiers)
    for file in os.listdir(src_folder):
        if file.endswith('.csv'):
            src_file = os.path.join(src_folder, file)
            dest_file = os.path.join(dest_folder, file)

            # D√©placer le fichier CSV
            shutil.move(src_file, dest_file)
            st.write(f"‚úÖ Fichier d√©plac√© : {src_file} -> {dest_file}")

    st.write(f"‚úÖ Tous les fichiers CSV ont √©t√© d√©plac√©s vers : {dest_folder}")

# Fonction permettant la t√©l√©chargement des datasets avec une barre de progression
def download_file_with_progress(year):
    # Configuration de la requ√™te
    base_url = "https://co2cars.apps.eea.europa.eu/tools/download"
    query_url = f"http://co2cars.apps.eea.europa.eu/10?source=%7B%22track_total_hits%22%3Atrue%2C%22query%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22constant_score%22%3A%7B%22filter%22%3A%7B%22bool%22%3A%7B%22must%22%3A%5B%7B%22bool%22%3A%7B%22should%22%3A%5B%7B%22term%22%3A%7B%22year%22%3A{year}%7D%7D%5D%7D%7D%2C%7B%22bool%22%3A%7B%22should%22%3A%5B%7B%22term%22%3A%7B%22scStatus%22%3A%22Final%22%7D%7D%5D%7D%7D%5D%7D%7D%7D%7D%5D%7D%7D%2C%22display_type%22%3A%22tabular%22%7D"
    download_url = f"{base_url}?download_query={query_url}&download_format=csv"

    headers = {
        "Referer": "https://co2cars.apps.eea.europa.eu/",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    # Stream the response to avoid loading everything in memory
    with requests.get(download_url, headers=headers, stream=True) as response:
        response.raise_for_status()
        
        # Taille totale du fichier
        total_size = int(response.headers.get('content-length', 0))
        total_size = int(total_size) if total_size is not None else 0

        block_size = 1024  # Taille de chaque chunk
        
        # Affichage de la barre de progression
        # progress_bar = st.progress(0)
        status_text = st.empty()
        downloaded_size = 0

        # Nom du fichier √† sauvegarder    
        RAW_DIR = "./data/raw"
        raw_dir = RAW_DIR
        os.makedirs(raw_dir, exist_ok=True)
        file_name = os.path.join(raw_dir, f"DF_Full_Raw_{year}.csv")

        with open(file_name, 'wb') as file:
            for data in response.iter_content(block_size):
                file.write(data)
                downloaded_size += len(data)

                # Formatage de la taille t√©l√©charg√©e
                formatted_downloaded = format_size(downloaded_size)

                # Mise √† jour de la progression
                if total_size > 0:
                    progress = int(downloaded_size * 100 / total_size)
                    # progress_bar.progress(min(progress, 100))
                    formatted_total = format_size(total_size)
                    status_text.text(f"üîÑ T√©l√©charg√© : {formatted_downloaded} / {formatted_total} ({progress}%)")
                    status_text.text(f"üîÑ T√©l√©chargement : {progress}%")
                else:
                    # Si la taille totale est inconnue, afficher le nombre d'octets t√©l√©charg√©s
                    status_text.text(f"üîÑ T√©l√©chargement : {formatted_downloaded} t√©l√©charg√©s")

        return file_name

# Fonction permettant la t√©l√©chargement des datasets avec une requ√™te SQL
def download_data_sql(selected_year):
    table_list = {
        '2021': 'co2cars_2021Fv24',
        '2022': 'co2cars_2022Fv26',
        '2023': 'co2cars_2023Fv28',
        '2024': 'co2cars_2024Fv30',
        '2025': 'co2cars_2025Fv32',
        '2026': 'co2cars_2026Fv34',
        '2027': 'co2cars_2027Fv36',
        '2028': 'co2cars_2028Fv38',
        '2029': 'co2cars_2029Fv40'
    }

    table = table_list[selected_year]

    query = f"""
    SELECT DISTINCT [Year] AS Year, Mk, Cn, [M (kg)], [Ewltp (g/km)], Ft, [Ec (cm3)], [Ep (KW)], [Erwltp (g/km)], Fc
    FROM [CO2Emission].[latest].[{table}]
    WHERE Mk IS NOT NULL 
      AND Cn IS NOT NULL 
      AND [M (kg)] IS NOT NULL
      AND [Ewltp (g/km)] IS NOT NULL
      AND Ft IS NOT NULL
      AND [Ec (cm3)] IS NOT NULL
      AND [Ep (KW)] IS NOT NULL
      AND [Erwltp (g/km)] IS NOT NULL
      AND [Year] IS NOT NULL
      AND Fc IS NOT NULL
    """

    encoded_query = urllib.parse.quote(query)
    page = 1
    records = []

    while True:
        url = f"https://discodata.eea.europa.eu/sql?query={encoded_query}&p={page}&nrOfHits=100000"
        response = requests.get(url)
        data = response.json()
        new_records = data.get("results", [])
        if not new_records:
            break
        records.extend(new_records)
        page += 1

    df = pd.DataFrame(records)
    return df

# D√©placement des fichiers puis suppression des dossiers
def move_contents_and_remove_folder(src_folder):
    """
    D√©place tous les fichiers et dossiers du dossier source vers le r√©pertoire courant,
    puis supprime le dossier source.

    :param src_folder: Chemin du dossier source.
    """
    try:
        # V√©rifier si le dossier source existe
        if not os.path.exists(src_folder):
            st.write(f"‚ùå Le dossier source {src_folder} n'existe pas.")
            return
        
        # R√©pertoire de destination (r√©pertoire courant)
        dest_folder = os.getcwd()

        # D√©placement des fichiers et dossiers
        for item in os.listdir(src_folder):
            src_path = os.path.join(src_folder, item)
            dest_path = os.path.join(dest_folder, item)
            
            # Si le fichier ou dossier existe d√©j√†, fusionner les dossiers ou √©craser les fichiers
            if os.path.exists(dest_path):
                if os.path.isdir(src_path):
                    shutil.copytree(src_path, dest_path, dirs_exist_ok=True)
                    shutil.rmtree(src_path)
                else:
                    shutil.copy2(src_path, dest_path)
                    os.remove(src_path)
            else:
                shutil.move(src_path, dest_path)

        # Supprimer le dossier source s'il est vide
        if not os.listdir(src_folder):
            os.rmdir(src_folder)
            st.write(f"‚úÖ Dossier source {src_folder} supprim√© avec succ√®s.")
        else:
            st.write(f"‚ö†Ô∏è Le dossier source {src_folder} n'a pas pu √™tre supprim√© car il n'est pas vide.")

        st.write(f"‚úÖ Contenu d√©plac√© vers {dest_folder}")

    except Exception as e:
        st.write(f"‚ùå Erreur : {e}")

# Charger le dataset
df, X_train, X_test, y_train, y_test = load_and_preprocess_data()

# Configuration de MLflow
mlflow.set_tracking_uri("http://mlflow:8080")
mlflow.set_experiment("MLflow Streamlit")

# D√©finir les onglets
tabs = ["T√©l√©chargement", "Exploration", "Entra√Ænement", "Historique", "Chargement", "Interface", "Lancement", "Utilisateur Final"]

# Cr√©ation des onglets
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs(tabs)

# Onglet 1 : R√©cup√©ration des Datasets
with tab1:
    st.header("üíΩ R√©cup√©ration du Projet via GitHub")

    if st.checkbox("T√©l√©chargement du GitHub") :

        # Configuration du dossier GitHub (1 : dossier priv√©, 0 : dossier public)
        private = 0

        # Nom du fichier ZIP √† sauvegarder
        zip_path = "c02_repo.zip"

        # Dossier d'extraction
        extract_to = "OCT24_MLOPS_CO2"
        
        # T√©l√©chargement du dossier GitHub via GitHub si public
        if private == 0:
            # Mesurer le temps de d√©but
            start_time = time.time()

            # URL du d√©p√¥t GitHub
            url_repo = "https://github.com/tdal38/OCT24_MLOPS_CO2/archive/refs/heads/main.zip"

            # T√©l√©charger le fichier ZIP
            response = requests.get(url_repo, stream=True)
            if response.status_code == 200:
                with open(zip_path, "wb") as file:
                    for chunk in response.iter_content(chunk_size=1024):
                        file.write(chunk)
                # Mesurer le temps de fin
                end_time = time.time()
                
                # Calculer et afficher la dur√©e
                execution_time = end_time - start_time
                
                st.write(f"‚úÖ T√©l√©chargement du dossier GitHub termin√© en {execution_time} secondes !")
            else:
                st.write("‚ùå Erreur lors du t√©l√©chargement du dossier GitHub.")

        # T√©l√©chargement du dossier GitHub via Google Drive si GitHub priv√©
        elif private == 1:
            # Mesurer le temps de d√©but
            start_time = time.time()

            # ID du fichier Google Drive
            file_id_repo = "1Ne1VJWQX4ixc29jIscJApEBp-YzY0kcB"
            # URL de t√©l√©chargement direct
            url_repo = f"https://drive.google.com/uc?id={file_id_repo}"

            # T√©l√©chargement du fichier
            gdown.download(url_repo, zip_path, quiet=False)

            # Mesurer le temps de fin
            end_time = time.time()

            # Calculer et afficher la dur√©e
            execution_time = end_time - start_time

            # Confirmation de t√©l√©chargement - 5s
            st.write(f"‚úÖ T√©l√©chargement de {zip_path} s'est termin√© en {round(execution_time,2)} secondes")

        # Mesurer le temps de d√©but
        start_time = time.time()

        # Extraire le fichier ZIP
        if os.path.exists(zip_path):
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                zip_ref.extractall(extract_to)

            # Supprimer le fichier ZIP apr√®s extraction
            os.remove(zip_path)
            # Mesurer le temps de fin
            end_time = time.time()
            # Calculer et afficher la dur√©e
            execution_time = end_time - start_time

            st.write(f"‚úÖ Extraction termin√©e dans le dossier '{extract_to}' r√©alis√© en {execution_time:.2f} seconde !")
        else:
            st.write("‚ùå Le fichier ZIP n'a pas √©t√© trouv√©.")

        # D√©finir le chemin du dossier source
        source_folder = "./OCT24_MLOPS_CO2"

        # V√©rifier si le dossier existe
        if not os.path.exists(source_folder):
            st.write("Le dossier source n'existe pas.")
        else:
            # D√©finir le dossier parent (le dossier pr√©c√©dent)
            parent_folder = os.path.dirname(os.path.abspath(source_folder))

            # Lister les fichiers et dossiers et les d√©placer
            for item in os.listdir(source_folder):
                source_path = os.path.join(source_folder, item)
                destination_path = os.path.join(parent_folder, item)

                # D√©placer les fichiers et dossiers
                shutil.move(source_path, destination_path)

            # Supprimer le dossier vide
            os.rmdir(source_folder)

    if st.checkbox("Pr√©paration des dossiers") :
        src = r"OCT24_MLOPS_CO2-main"
        move_contents_and_remove_folder(src)
        st.write(f"‚úÖ Traitement des dossiers et fichiers de GitHub termin√©e !")

        if os.path.exists("OCT24_MLOPS_CO2-main"):
            shutil.rmtree("OCT24_MLOPS_CO2-main")
            st.write("‚úÖ Suppression du dossier inutile")
        else:
            st.write("‚ÑπÔ∏è Le dossier 'OCT24_MLOPS_CO2-main' n'existe pas, aucune suppression n√©cessaire.")

    st.header("üì• R√©cup√©ration des Datasets")

# --- T√©l√©chargement du CSV complet avec choix des dates --- # 
    st.write("T√©l√©chargement du CSV complet avec choix des dates")
    # S√©lection de l'ann√©e
    selected_year = st.selectbox("S√©lectionnez l'ann√©e :", list(range(2010, 2030)))

    if st.button("T√©l√©charger le fichier CSV"):
        # Mesurer le temps de d√©but
        start_time = time.time()
        # T√©l√©chargement du fichier
        file_path = download_file_with_progress(selected_year)

        if file_path:
            with open(file_path, "rb") as f:
                st.download_button(label="T√©l√©charger le fichier", data=f, file_name=os.path.basename(file_path), mime="text/csv")
            st.success(f"‚úÖ Fichier t√©l√©charg√© : {file_path}")
        else:
            st.error("‚ùå √âchec du t√©l√©chargement du fichier.")

        # Mesurer le temps de fin
        end_time = time.time()

        # Calculer et afficher la dur√©e
        execution_time = end_time - start_time

        # Confirmation de t√©l√©chargement
        st.write(f"‚úÖ T√©l√©chargement de {file_path} s'est termin√© en {round(execution_time,2)} secondes")

# --- T√©l√©chargement du CSV avec requ√™te SQL format√©e --- # 
    st.write("T√©l√©chargement du CSV avec requ√™te SQL format√©e")
    selected_year = st.selectbox('Choisissez une ann√©e :', [str(year) for year in range(2021, 2030)])

    if st.button('T√©l√©charger les donn√©es'):
        with st.spinner('T√©l√©chargement en cours...'):
            df = download_data_sql(selected_year)
            if not df.empty:
                RAW_DIR = "./data/raw"
                raw_dir = RAW_DIR
                os.makedirs(raw_dir, exist_ok=True)
                output_filepath = os.path.join(raw_dir, f"DF_SQL_Raw_{selected_year}.csv")
                df.to_csv(output_filepath, index=False)
                st.success(f"‚úÖ Les donn√©es ont √©t√© t√©l√©charg√©es avec succ√®s et enregistr√©es dans {output_filepath}.")
            else:
                st.error("‚ùå Aucune donn√©e n'a √©t√© r√©cup√©r√©e.")
    
    st.header("üóÇÔ∏è D√©placement des fichiers CSV")
    if st.checkbox("D√©placer les fichiers"):
        source_directory = r"./"
        destination_directory = r"./data"

        move_csv_files(source_directory, destination_directory)

# Onglet 2 : Exploration du dataset
with tab2:
    st.header("üìä Exploration du Dataset")

    # Affichage d'un aper√ßu du dataset
    st.subheader("Aper√ßu du Dataset üìã")
    st.dataframe(df.head(10))  # Afficher les 10 premi√®res lignes

# Onglet 3 : Entra√Ænement des mod√®les
with tab3:
    st.header("üèãÔ∏è‚Äç‚ôÇÔ∏è Entra√Ænement des Mod√®les")

    # Affichage du mod√®le
    st.subheader("Choix du mod√®le üìã")

    # S√©lection du mod√®le
    model_choice = st.selectbox(
        "Choisissez un mod√®le de Machine Learning",
        ["R√©gression Lin√©aire", "KNN", "Random Forest"]
    )

    # Afficher le choix s√©lectionn√©
    st.write(f"üîç Mod√®le s√©lectionn√© : **{model_choice}**")

    # üìå Entra√Ænement et Logging MLflow
    if st.button("Entra√Æner le mod√®le"):
        
        with mlflow.start_run():
            
            # Mod√®le s√©lectionn√©
            if model_choice == "R√©gression Lin√©aire":
                model = LinearRegression()
            elif model_choice == "KNN":
                model = KNeighborsRegressor()
            elif model_choice == "Random Forest":
                model = RandomForestRegressor()
            
            # Entra√Ænement
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            # Calcul des m√©triques
            r2 = r2_score(y_test, y_pred)
            rmse = mean_squared_error(y_test, y_pred)

            # Log dans MLflow
            mlflow.log_metric("R2", r2)
            mlflow.log_metric("RMSE", rmse)

            # MAJ des m√©triques Prometheus
            r2_gauge.set(r2)
            rmse_gauge.set(rmse)

            # Ajouter un tag d'info
            mlflow.set_tag("Training Info", f"Mod√®le {model_choice} entra√Æn√© sur DF2023")

            # Enregistrer le mod√®le MLflow
            signature = infer_signature(X_train, model.predict(X_train))
            mlflow.sklearn.log_model(
                sk_model=model,
                artifact_path="test_model",
                signature=signature,
                input_example=X_train,
                registered_model_name="tracking-quickstart",
            )

            # Affichage des r√©sultats
            st.write(f"### üìä R√©sultats du Mod√®le : {model_choice}")
            st.write(f"**R¬≤ :** {r2:.4f}") #.4f sert √† avoir 4 chiffres apres la virgule
            st.write(f"**RMSE :** {rmse:.4f}") 

            # Affichage des valeurs r√©elles vs pr√©dictions
            result_df = pd.DataFrame({"Valeur R√©elle": y_test, "Pr√©diction": y_pred})
            st.dataframe(result_df.head(10))

# Onglet 4 : Historique des Runs
with tab4:
    st.header("üìú Historique des Runs MLflow")

    # Se connecter √† MLflow et r√©cup√©rer les runs tri√©s par date
    mlflow.set_experiment("MLflow Streamlit")
    runs = mlflow.search_runs(order_by=["start_time DESC"])  # Tri des runs du plus r√©cent au plus ancien

    if runs.empty:
        st.info("Aucun mod√®le enregistr√© pour l'instant. Lance un entra√Ænement !")
    else:
        # Extraire les informations clefs des runs
        df_runs = runs[["run_id", "metrics.R2", "metrics.RMSE", "start_time", "tags.mlflow.runName"]]
        df_runs.columns = ["Run ID", "R¬≤ Score", "RMSE", "Date", "Nom du mod√®le"]

        # S√©lectionner la derni√®re run par d√©faut
        latest_run_id = df_runs.iloc[0]["Run ID"]  # Premier √©l√©ment (le plus r√©cent)
        selected_run = st.selectbox("Affichage de la derni√®re Run :", list(df_runs["Nom du mod√®le"]), index=0)   

        # Afficher les d√©tails de la run s√©lectionn√©e
        run_details = df_runs[df_runs["Nom du mod√®le"] == selected_run]
        st.write(f"üìå **Nom du mod√®le** : {run_details['Nom du mod√®le'].values[0]}")
        st.write(f"üìÇ **Run ID** : {run_details['Run ID'].values[0]}")
        st.write(f"üìÖ **Date** : {run_details['Date'].values[0]}")
        st.write(f"üìä **R¬≤ Score** : {run_details['R¬≤ Score'].values[0]:.4f}")
        st.write(f"üìâ **RMSE** : {run_details['RMSE'].values[0]:.4f}")

# Onglet 5 : Chargement des mod√®les
with tab5:
    st.header("üöÄ Charger un Mod√®le MLflow et Pr√©dire")

    mlflow.set_experiment("MLflow Streamlit")
    runs = mlflow.search_runs(order_by=["start_time DESC"])  # Trier du plus r√©cent au plus ancien

    if runs.empty:
        st.info("Aucun mod√®le enregistr√© pour l'instant. Lance un entra√Ænement !")
    else:
        # Extraire les informations cl√©s des mod√®les enregistr√©s
        df_models = runs[["run_id", "tags.mlflow.runName"]]
        df_models.columns = ["Run ID", "Nom du Mod√®le"]

        # S√©lectionner un mod√®le via son nom
        selected_model_name = st.selectbox("üéØ Choisissez un Mod√®le :", df_models["Nom du Mod√®le"], index=0)
        selected_run_id = df_models[df_models["Nom du Mod√®le"] == selected_model_name]["Run ID"].values[0]  # R√©cup√©ration du Run ID correspondant

        # Charger le mod√®le MLflow s√©lectionn√©
        model_uri = f"runs:/{selected_run_id}/test_model"
        loaded_model = mlflow.pyfunc.load_model(model_uri)

        st.success(f"‚úÖ Mod√®le '{selected_model_name}' charg√© avec succ√®s !")

        # Formulaire pour entrer les caract√©ristiques du v√©hicule
        st.write("### üèéÔ∏è Entrez les caract√©ristiques du v√©hicule")
        with st.form("prediction_form"):
            col1, col2 = st.columns(2)
            with col1:
                m_kg = st.number_input("Masse du v√©hicule (kg)", min_value=500, max_value=3000, step=1)
                ec_cm3 = st.number_input("Cylindr√©e (cm¬≥)", min_value=500, max_value=6000, step=1)
            with col2:
                ep_kw = st.number_input("Puissance (kW)", min_value=20, max_value=500, step=1)
                erwltp = st.number_input("R√©duction d'√©missions WLTP (g/km)", min_value=0.0, max_value=5.0, step=0.01)

            fuel_consumption = st.number_input("Consommation de carburant (L/100km)", min_value=2.0, max_value=15.0, step=0.1)
            ft = st.selectbox("Type de carburant", ["Diesel", "Essence"])
            fuel_types = {"Diesel": [1, 0], "Essence": [0, 1]}
            ft_encoded = fuel_types[ft]

            # Construction de l'input (features utilis√©es lors de l'entra√Ænement)
            input_values = [m_kg, ec_cm3, ep_kw, erwltp, fuel_consumption] + ft_encoded
            input_data_df = pd.DataFrame([input_values], columns=['M (kg)', 'Ec (cm3)', 'Ep (KW)', 'Erwltp (g/km)', 'Fc', 'Ft_Diesel', 'Ft_Essence'])
            input_data_df = input_data_df.astype({
                "M (kg)": int,
                "Ec (cm3)": int,
                "Ep (KW)": int,
                "Erwltp (g/km)": float,
                "Fc": float,
                "Ft_Diesel": int,
                "Ft_Essence": int,
                })

            submitted = st.form_submit_button("üîé Pr√©dire")

        # Faire la pr√©diction lorsque le formulaire est soumis
        if submitted:
            prediction = loaded_model.predict(input_data_df)
            st.success(f"üìä **Pr√©diction des √©missions de CO‚ÇÇ : {prediction[0]:.2f} g/km**")

# Onglet 6 : Interface MLflow avec Prometheus
with tab6:
    st.header("üìä Interface MLflow avec Prometheus")

    # Requ√™te http vers Prometheus
    url = "http://prometheus:9090/api/v1/query"
    params = {"query": "model_r2_score"}

    response = requests.get(url, params=params)

    if response.ok:
        data = response.json()
        st.write("‚úÖ R√©ponse re√ßue :", data)
    else:
        st.error("‚ùå √âchec de la requ√™te Prometheus")

    if response.ok and data["data"]["result"]:
        # On r√©cup√®re la valeur R¬≤ (indice 1 dans le tableau "value")
        r2_value = float(data["data"]["result"][0]["value"][1])
        
        # On affiche un df avec la derni√®re requ√™te Prometheus
        df = pd.DataFrame({
            "timestamp": [datetime.datetime.now()],
            "R2 Score": [r2_value]
        })

        st.write("üìä Derni√®re requ√™te Prometheus :", df)

    else:
        st.info("‚ÑπÔ∏è Aucune donn√©e R¬≤ disponible actuellement.")


    # On enregistre localement la derni√®re requ√™te Prometheus
    st.subheader("üì• Enregistrement manuel depuis Prometheus")

    if st.button("Enregistrer la m√©trique actuelle"):
        url = "http://prometheus:9090/api/v1/query"
        params = {"query": "model_r2_score"}
        response = requests.get(url, params=params)

        if response.ok and response.json()["data"]["result"]:
            result = response.json()["data"]["result"][0]
            timestamp_raw = float(result["value"][0])
            value = float(result["value"][1])
            timestamp = datetime.datetime.fromtimestamp(timestamp_raw)

            # Ignorer si valeur = 0 (aucun entra√Ænement encore fait)
            if value == 0.0:
                st.warning("‚ö†Ô∏è La m√©trique est √† 0. Aucun entra√Ænement r√©cent d√©tect√©.")
                st.stop()

            st.success(f"üìà Valeur r√©cup√©r√©e : R¬≤ = {value:.4f} √† {timestamp}")

            # Chemin vers le fichier
            csv_path = os.path.join("data", "metrics_history.csv")

            # Cr√©er le fichier avec ent√™te si besoin (si non existant)
            if not os.path.exists(csv_path):
                with open(csv_path, mode='w', newline='') as file:
                    writer = csv.writer(file)
                    writer.writerow(["timestamp", "R2 Score"])

            # V√©rifier si la valeur est d√©j√† dans le fichier (pour √©viter les R¬≤ doublons) et l'enregistre
            already_logged = False
            if os.path.exists(csv_path):
                try:
                    df_existing = pd.read_csv(csv_path)
                    already_logged = not df_existing[df_existing["R2 Score"] == value].empty
                except pd.errors.EmptyDataError:
                    pass

            if already_logged:
                st.info("‚ÑπÔ∏è Cette valeur R¬≤ a d√©j√† √©t√© enregistr√©e. Aucun ajout.")
            else:
                with open(csv_path, mode='a', newline='') as file:
                    writer = csv.writer(file)
                    writer.writerow([timestamp.isoformat(), value])
                st.success("‚úÖ Nouvelle valeur enregistr√©e.")
        else:
            st.warning("‚ö†Ô∏è Aucune donn√©e disponible depuis Prometheus.")

    # Graph/Historique des R¬≤ √† partir du fichier CSV
    st.subheader("üìä Historique des scores R¬≤ enregistr√©s")

    csv_path = os.path.join("data", "metrics_history.csv")

    if os.path.exists(csv_path):
        try:
            df_metrics = pd.read_csv(csv_path)

            # V√©rifie qu‚Äôil y a bien des donn√©es
            if not df_metrics.empty:
                df_metrics["timestamp"] = pd.to_datetime(df_metrics["timestamp"])
                fig = px.line(df_metrics, x="timestamp", y="R2 Score", title="√âvolution du score R¬≤ dans le temps")
                st.plotly_chart(fig)
            else:
                st.info("üì≠ Le fichier CSV existe mais ne contient aucune donn√©e pour le moment.")
        except Exception as e:
            st.warning(f"‚ùå Erreur lors de la lecture du fichier : {e}")
    else:
        st.info("üìÅ Aucune donn√©e disponible (le fichier CSV n'existe pas encore).")

# Onglet 7 : Lancement de la Pipeline depuis DVC
with tab7:
    st.header("‚öôÔ∏è Lancement de la Pipeline depuis DVC")

    if st.button('Ex√©cuter la Pipeline'):
        with st.spinner('Ex√©cution de la commande en cours...'):
            run_dvc_repro()

# Onglet 8 : Utilisateur final
with tab8:
    # Configuration des requ√™tes
    load_dotenv()
    API_URL = "http://127.0.0.1:8000"
    TOKEN_URL = f"{API_URL}/token"
    PREDICT_URL = f"{API_URL}/predict"

    # Interface
    st.header("üöó Pr√©diction d'√©missions de CO2")

    # Authentification
    st.sidebar.header("üîê Authentification")
    username = st.sidebar.text_input("Nom d'utilisateur")
    password = st.sidebar.text_input("Mot de passe", type="password")
    login_button = st.sidebar.button("Se connecter")

    # Gestion du token
    if "token" not in st.session_state:
        st.session_state.token = None

    if login_button:
        try:
            response = requests.post(
                TOKEN_URL,
                data={"username": username, "password": password},
                headers={"Content-Type": "application/x-www-form-urlencoded"}
            )
            if response.status_code == 200:
                st.session_state.token = response.json()["access_token"]
                st.sidebar.success("Connexion r√©ussie !")
            else:
                st.sidebar.error("Identifiants incorrects")
        except Exception as e:
            st.sidebar.error(f"Erreur de connexion: {str(e)}")

    # Formulaire de pr√©diction
    if st.session_state.token:
        with st.form("prediction_form_api"):
            st.header("üìä Caract√©ristiques du v√©hicule")
            
            col1, col2 = st.columns(2)
            with col1:
                M_kg = st.number_input("Poids (kg)", min_value=500, max_value=5000, value=1500)
                Ec_cm3 = st.number_input("Cylindr√©e (cm3)", min_value=500, max_value=8000, value=2000)
                Ep_KW = st.number_input("Puissance (KW)", min_value=20, max_value=1000, value=100)
            
            with col2:
                Erwltp_g_km = st.number_input("Emission RWltp (g/km)", min_value=0.0, max_value=5.0, value=0.01)
                Fc = st.number_input("Consommation carburant", min_value=0.0, max_value=20.0, value=6.5)
                Ft = st.selectbox("Type de carburant", ['Essence', 'Diesel', 'GPL'])

            # Charger le fichier DF_Processed.csv pour r√©cup√©rer les colonnes manquantes
            data_path = "data/processed/DF_Processed.csv"
            if not os.path.exists(data_path):
                st.error(f"Le fichier de donn√©es {data_path} n'existe pas.")
                st.stop()

            # Charger les donn√©es pr√©trait√©es pour r√©cup√©rer la structure des colonnes
            df_final = pd.read_csv(data_path)
            X_final = df_final.drop(['Ewltp (g/km)', 'Cn', 'Year'], axis=1)
            all_columns = X_final.columns.tolist()  # Liste compl√®te des colonnes d'origine
            marques = [col.replace('Mk_', '') for col in all_columns if col.startswith('Mk_')]
            Mk = st.selectbox("Marque (Mk)", marques)

            submit_button = st.form_submit_button("Pr√©dire les √©missions")

        if submit_button:
            try:
                # Cr√©ation du payload
                payload = {
                    "M (kg)": M_kg,
                    "Ec (cm3)": Ec_cm3,
                    "Ep (KW)": Ep_KW,
                    "Erwltp (g/km)": Erwltp_g_km,
                    "Fc": Fc,
                    "Ft": Ft,
                    "Mk": Mk
                }
                
                headers = {"Authorization": f"Bearer {st.session_state.token}"}
                response = requests.post(PREDICT_URL, json=payload, headers=headers)
                
                if response.status_code == 200:
                    prediction = response.json()["prediction"]
                    st.success(f"### üîç R√©sultat : {prediction:.2f} g/km")
                else:
                    st.error(f"Erreur API: {response.text}")
                    
            except Exception as e:
                st.error(f"Erreur lors de la pr√©diction: {str(e)}")

    else:
        st.warning("Veuillez vous authentifier pour acc√©der aux pr√©dictions")