{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1716cb90",
   "metadata": {},
   "source": [
    "Importation des librairies : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c890fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89164ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da21338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dagshub\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b85934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la librairie permettant la sauvegarde des fichiers de log : \n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
    "from logging_script import setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68569a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du logger : \n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion avec MLFlow pour le suivi des exp√©rimentations et l'enregistrement du mod√®le : \n",
    "dagshub.init(\n",
    "    repo_owner=\"tiffany.dalmais\",\n",
    "    repo_name=\"OCT24_MLOPS_CO2\",\n",
    "    mlflow=True\n",
    ")\n",
    "mlflow.autolog()\n",
    "with mlflow.start_run():\n",
    "    logger.info(\"‚úÖ Entra√Ænement du mod√®le d√©marr√© avec succ√®s (modelisation.py).\")\n",
    "\n",
    "    # Importation de la configuration des chemins : \n",
    "    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\")))\n",
    "    import config\n",
    "\n",
    "    # Chargement du nom du fichier de donn√©es pr√©trait√©es :\n",
    "    processed_file_path = os.path.join(config.PROCESSED_DIR, \"DF_Processed.csv\")\n",
    "\n",
    "    if not os.path.exists(processed_file_path):\n",
    "        logger.error(f\"‚ùå Le fichier {processed_file_path} n'existe pas.\")\n",
    "    else:\n",
    "        try:\n",
    "            df = pd.read_csv(processed_file_path)\n",
    "            logger.info(f\"‚öôÔ∏è Fichier .csv charg√© avec succ√®s : ({len(df)} lignes).\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur lors du chargement du fichier .csv : {e}.\")\n",
    "\n",
    "    # S√©paration de X et y :\n",
    "    try:\n",
    "        X = df.drop(['Ewltp (g/km)', 'Cn', 'Year'], axis=1)\n",
    "        y = df['Ewltp (g/km)']\n",
    "        logger.info(\"‚úÖ S√©paration de X et y r√©ussie.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erreur lors de la s√©paration des variables : {e}.\")\n",
    "\n",
    "    # Split train/test :\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    logger.info(\"‚úÖ Split train/test effectu√©.\")\n",
    "\n",
    "    # Normalisation :\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    logger.info(\"‚úÖ Donn√©es normalis√©es avec StandardScaler.\")\n",
    "\n",
    "    # Mod√®le final - RandomForestRegressor :\n",
    "    model_final = RandomForestRegressor(bootstrap=False, max_features=0.75, min_samples_leaf=1,\n",
    "                                        min_samples_split=9, n_estimators=100, random_state=42)\n",
    "    model_final.fit(X_train_scaled, y_train)\n",
    "    results_model_final = model_final.predict(X_test_scaled)\n",
    "    logger.info(\"‚úÖ Mod√®le RandomForest entra√Æn√© avec succ√®s.\")\n",
    "\n",
    "    # Enregistrement du mod√®le dans MLFlow : \n",
    "    params = {\n",
    "    \"bootstrap\": False,\n",
    "    \"max_features\": 0.75,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 9,\n",
    "    \"n_estimators\": 100,\n",
    "    \"random_state\": 42\n",
    "    }\n",
    "    signature = infer_signature(X_test_scaled, results_model_final)\n",
    "\n",
    "    # Affichage et enregistrement des metrics : \n",
    "    # Import des fonctions cr√©√©es dans le fichier metrics.py : \n",
    "    from src.utils.metrics import compute_metrics, save_metrics\n",
    "\n",
    "    # Calcul et affichage des metrics : \n",
    "    metrics = compute_metrics(y_test, results_model_final)\n",
    "    logger.info(f\"üìä RMSE: {metrics['rmse']:.4f} | R¬≤: {metrics['r2']:.4f}\")\n",
    "\n",
    "    # Enregistrement des metrics : \n",
    "    os.makedirs(config.OUTPUTS_DIR, exist_ok=True)\n",
    "    metrics_file = os.path.join(config.OUTPUTS_DIR, \"metrics.json\")\n",
    "    save_metrics(metrics, metrics_file)\n",
    "    logger.info(f\"‚úÖ Fichier des m√©triques enregistr√© : {metrics_file}.\")\n",
    "\n",
    "    # Enregistrement des param√®tres et metrics : \n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"rmse\", metrics[\"rmse\"])\n",
    "    mlflow.log_metric(\"r2\", metrics[\"r2\"])\n",
    "    logger.info(f\"‚úÖ Enregistrement des m√©triques et des param√®tres effectu√©.\")\n",
    "    \n",
    "    # Enregistrement du mod√®le : \n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model_final,\n",
    "        artifact_path=\"sklearn-model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"RandomForest_Final\",\n",
    "    )\n",
    "    logger.info(\"üìÅ Mod√®le sauvegard√© sur MLFlow.\")\n",
    "\n",
    "    # Analyse des erreurs : \n",
    "    df_results_final = pd.DataFrame({'y_true': y_test, 'y_pred': results_model_final})\n",
    "    df_results_final['error'] = abs(df_results_final['y_true'] - df_results_final['y_pred'])\n",
    "    seuil = 20\n",
    "    outliers = df_results_final[df_results_final['error'] > seuil]\n",
    "    logger.info(f\"üìå Affichage des √©carts de pr√©diction importants : \\n {outliers.describe()}\")\n",
    "\n",
    "    # Cr√©ation de la variable contenant le nom du mod√®le : \n",
    "    model_filename = \"RandomForest_Final.pkl\"\n",
    "\n",
    "    # Enregistrement du mod√®le : \n",
    "    # D√©finition du chemin :\n",
    "    models_dir = config.MODELS_DIR\n",
    "\n",
    "    # Cr√©ation du dossier s'il n'existe pas :\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "        logger.info(\"üóÇÔ∏è Dossier de sauvegarde du mod√®le v√©rifi√© ou cr√©√© avec succ√®s.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f'‚ùå Erreur lors de la cr√©ation du dossier \"models\" : {e}.')\n",
    "\n",
    "    # Construction du chemin complet vers le fichier dans le dossier \"models\" existant :\n",
    "    model_path = os.path.join(models_dir, model_filename)\n",
    "\n",
    "    # Enregistrement du mod√®le entra√Æn√© : \n",
    "    joblib.dump(model_final, model_path)\n",
    "    logger.info(f\"üìÅ Mod√®le sauvegard√© localement : {model_path}.\")\n",
    "\n",
    "    # Enregistrement des variables utilis√©es pour l'entra√Ænement du mod√®le : \n",
    "    features_filename = \"columns_list.pkl\"\n",
    "    feature_path = os.path.join(models_dir, features_filename)\n",
    "    columns_list = X.columns.tolist()\n",
    "    joblib.dump(columns_list, feature_path)\n",
    "    logger.info(\"üìÅ Le fichier 'columns_list.pkl' a √©t√© g√©n√©r√© avec succ√®s !\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
